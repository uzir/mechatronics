import streamlit as st
import google.generativeai as genai
import time
import PyPDF2
from PIL import Image

# --- ×¤×•× ×§×¦×™×” ×œ×§×¨×™××ª ×××’×¨ ×”×©××œ×•×ª ××”×§×•×‘×¥ ---
@st.cache_data
def load_knowledge_base(file_path):
    try:
        with open(file_path, "rb") as pdf_file:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text
    except FileNotFoundError:
        # This error is now less critical as it only affects the chat tab
        return None
    except Exception as e:
        st.error(f"×©×’×™××” ×‘×§×¨×™××ª ×§×•×‘×¥ ×”-PDF: {e}")
        return None

# --- ×”×’×“×¨×•×ª ×•×”×•×¨××•×ª ×œ×‘×•×˜ ---

# 1. ×˜×¢×™× ×ª ×”×™×“×¢ ××”×§×•×‘×¥ ×©×”×¢×œ×™× ×• ×œ-GitHub (×©× ×”×§×•×‘×¥ ×¢×•×“×›×Ÿ)
knowledge_base_text = load_knowledge_base("819387ALL.pdf")

# 2. ×”×”×•×¨××•×ª ×”×‘×¡×™×¡×™×•×ª ×œ×‘×•×˜ (×”×¢×ª×§ ×œ×›××Ÿ ××ª ×›×œ ×”×”×•×¨××•×ª ×”××¤×•×¨×˜×•×ª ×©×œ×š)
BASE_SYSTEM_INSTRUCTION = """
××ª×” ××•×¨×” ××•××—×” ×‘××’××•×ª ××›×˜×¨×•× ×™×§×” (×›×™×ª×•×ª ×™â€“×™"×‘) ×¢× ×©×œ×•×©×” ××¦×‘×™×... (×•×›×•')
"""

# 3. ×©×™×œ×•×‘ ×××’×¨ ×”×™×“×¢ ×‘×”×•×¨××•×ª ×œ××¢×¨×›×ª
if knowledge_base_text:
    SYSTEM_INSTRUCTION = f"""
    {BASE_SYSTEM_INSTRUCTION}
    ---
    **×××’×¨ ×™×“×¢ ×§×‘×•×¢:**
    {knowledge_base_text}
    ---
    """
else:
    SYSTEM_INSTRUCTION = BASE_SYSTEM_INSTRUCTION


PAGE_TITLE = "ğŸ¤– ×”××•×¨×” ×œ××›×˜×¨×•× ×™×§×”"

# --- ×”×’×“×¨×•×ª ×¢××•×“ ×•-UI ---
st.set_page_config(page_title=PAGE_TITLE, page_icon="ğŸ¤–", layout="wide")
st.markdown("""<style> body { direction: rtl; } .stTextInput > div > div > input { direction: rtl; } </style>""", unsafe_allow_html=True)
st.title(PAGE_TITLE)

# --- ×”×’×“×¨×•×ª ×”××•×“×œ ×•×”-API ---
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    model = genai.GenerativeModel(
        model_name="gemini-1.5-pro-latest",
        system_instruction=SYSTEM_INSTRUCTION,
        tools=['google_search_retrieval']
    )
    # Model for image/quiz generation that doesn't need the whole system prompt
    basic_model = genai.GenerativeModel(model_name="gemini-1.5-pro-latest")

except Exception as e:
    st.error("×©×’×™××” ×‘×”×’×“×¨×ª ×”-API Key.", icon="ğŸš¨")
    st.stop()

# --- ×”×’×“×¨×ª ×˜××‘×™× (×œ×©×•× ×™×•×ª) ---
tab_chat, tab_image, tab_quiz = st.tabs(["ğŸ’¬ ×¦'××˜ ×¢× ×”×‘×•×˜", "ğŸ–¼ï¸ × ×™×ª×•×— ×ª××•× ×•×ª", "ğŸ§  ××—×•×œ×œ ××‘×—× ×™×"])

# --- ×˜××‘ 1: ×¦'××˜ ×¨×’×™×œ ---
with tab_chat:
    st.header("×©×™×—×” ×¢× ×”××•×¨×” ×œ××›×˜×¨×•× ×™×§×”")
    if not knowledge_base_text:
        st.warning("×©×™× ×œ×‘: ×××’×¨ ×”×™×“×¢ ×”×§×‘×•×¢ (×§×•×‘×¥ ×”-PDF) ×œ× × ×˜×¢×Ÿ. ×”×‘×•×˜ ×™×¤×¢×œ ×¢×œ ×‘×¡×™×¡ ×”×™×“×¢ ×”×›×œ×œ×™ ×©×œ×• ×•×—×™×¤×•×© ×‘××™× ×˜×¨× ×˜.")
    
    INITIAL_MESSAGE = "×©×œ×•×, ×× ×™ ×”××•×¨×” ×”×“×™×’×™×˜×œ×™ ×œ××›×˜×¨×•× ×™×§×”. ×××’×¨ ×”×™×“×¢ ×©×œ×™ ×˜×¢×•×Ÿ ×•××•×›×Ÿ. ××™×š ××•×›×œ ×œ×¢×–×•×¨?"
    if "chat" not in st.session_state:
        st.session_state.chat = model.start_chat(history=[])
        st.session_state.messages = [{"role": "assistant", "content": INITIAL_MESSAGE}]

    for message in st.session_state.get("messages", []):
        with st.chat_message(message["role"]):
            st.markdown(f'<div style="direction: rtl;">{message["content"]}</div>', unsafe_allow_html=True)

    if prompt := st.chat_input("×›×ª×‘×• ×›××Ÿ ××ª ×©××œ×ª×›×..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(f'<div style="direction: rtl;">{prompt}</div>', unsafe_allow_html=True)
        
        with st.chat_message("assistant"):
            with st.spinner("×—×•×©×‘, ××¢×™×™×Ÿ ×‘×××’×¨ ×•×’× ××—×¤×© ×‘×¨×©×ª..."):
                response_stream = st.session_state.chat.send_message(prompt, stream=True)
                full_response = st.write_stream(response_stream)
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- ×˜××‘ 2: × ×™×ª×•×— ×ª××•× ×•×ª ---
with tab_image:
    st.header("× ×™×ª×•×— ×©×¨×˜×•×˜×™× ×•×ª××•× ×•×ª")
    st.info("×”×¢×œ×” ×ª××•× ×” ×©×œ ×©×¨×˜×•×˜ ×˜×›× ×™, ××¢×’×œ ×—×©××œ×™, ××• ×¨×›×™×‘, ×•×©××œ ××ª ×”×‘×•×˜ ×©××œ×” ×œ×’×‘×™×”.")
    
    uploaded_image = st.file_uploader("×‘×—×¨ ×§×•×‘×¥ ×ª××•× ×”", type=["png", "jpg", "jpeg"])
    image_prompt = st.text_input("××” ×ª×¨×¦×” ×œ×©××•×œ ×¢×œ ×”×ª××•× ×”?", key="image_q")

    if st.button("× ×ª×— ××ª ×”×ª××•× ×”"):
        if uploaded_image and image_prompt:
            with st.spinner("××¢×‘×“ ××ª ×”×ª××•× ×” ×•×× ×ª×—..."):
                try:
                    image_obj = Image.open(uploaded_image)
                    # Send both text and image to the model
                    response = basic_model.generate_content([image_prompt, image_obj])
                    st.subheader("×ª×•×¦××•×ª ×”× ×™×ª×•×—:")
                    st.markdown(f'<div style="direction: rtl;">{response.text}</div>', unsafe_allow_html=True)
                except Exception as e:
                    st.error(f"××™×¨×¢×” ×©×’×™××” ×‘×¢×™×‘×•×“ ×”×ª××•× ×”: {e}")
        else:
            st.warning("×× × ×”×¢×œ×” ×ª××•× ×” ×•×›×ª×•×‘ ×©××œ×”.")

# --- ×˜××‘ 3: ××—×•×œ×œ ××‘×—× ×™× ---
with tab_quiz:
    st.header("××—×•×œ×œ ××‘×—× ×™× ×•×—×™×“×•× ×™× ××™× ×˜×¨××§×˜×™×‘×™")
    st.info("×”×’×“×¨ ××ª ×”×¤×¨××˜×¨×™× ×”×¨×¦×•×™×™×, ×•×”×‘×•×˜ ×™×›×™×Ÿ ×¢×‘×•×¨×š ××‘×—×Ÿ ××•×ª×× ××™×©×™×ª.")

    with st.form("quiz_form"):
        quiz_topic = st.text_input("× ×•×©× ×”××‘×—×Ÿ (×œ×“×•×’××”: '×—×•×§ ××•×”× ×•××¢×’×œ×™× ×˜×•×¨×™×™×')")
        num_questions = st.number_input("××¡×¤×¨ ×©××œ×•×ª", min_value=1, max_value=20, value=5)
        question_type = st.selectbox("×¡×•×’ ×”×©××œ×•×ª", ["×¨×‘-×‘×¨×™×¨×ª×™×•×ª (×××¨×™×§××™×•×ª) ×¢× 4 ××¤×©×¨×•×™×•×ª", "×©××œ×•×ª ×¤×ª×•×—×•×ª", "×©××œ×•×ª × ×›×•×Ÿ / ×œ× × ×›×•×Ÿ"])
        difficulty = st.select_slider("×¨××ª ×§×•×©×™", options=["×™×¡×•×“×™×ª", "×‘×™× ×•× ×™×ª", "××ª×§×“××ª"])
        
        submitted = st.form_submit_button("ğŸš€ ×¦×•×¨ ××ª ×”××‘×—×Ÿ")

    if submitted:
        if quiz_topic:
            with st.spinner(f"××›×™×Ÿ ××‘×—×Ÿ ×‘×¨××” {difficulty} ×¢×œ {quiz_topic}..."):
                quiz_prompt = f"""
                ×¦×•×¨ ××‘×—×Ÿ ×¢×‘×•×¨ ×ª×œ××™×“×™ ××’××ª ××›×˜×¨×•× ×™×§×”.
                ×”× ×•×©×: {quiz_topic}
                ××¡×¤×¨ ×”×©××œ×•×ª: {num_questions}
                ×¡×•×’ ×”×©××œ×•×ª: {question_type}
                ×¨××ª ×§×•×©×™: {difficulty}
                
                ×”×¦×’ ××ª ×”××‘×—×Ÿ ×‘×¤×•×¨××˜ Markdown, ×›×•×œ×œ ×›×•×ª×¨×ª ×‘×¨×•×¨×”.
                ×× ×”×©××œ×•×ª ×”×Ÿ ×¨×‘-×‘×¨×™×¨×ª×™×•×ª, ×”×¦×’ 4 ××¤×©×¨×•×™×•×ª ×•×¡××Ÿ ××ª ×”×ª×©×•×‘×” ×”× ×›×•× ×” ×‘×¡×•×£ ×”××‘×—×Ÿ (×‘×§×˜×¢ × ×¤×¨×“ ×©×œ "××—×•×•×Ÿ ×ª×©×•×‘×•×ª").
                """
                response = basic_model.generate_content(quiz_prompt)
                st.subheader(f"××‘×—×Ÿ ×‘× ×•×©×: {quiz_topic}")
                st.markdown(f'<div style="direction: rtl;">{response.text}</div>', unsafe_allow_html=True)
        else:
            st.warning("×× × ×”×–Ğ½ × ×•×©× ×œ××‘×—×Ÿ.")
